import streamlit as st
import os
import json
import openai
import pandas as pd
from langchain.schema import Document, messages_from_dict
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.schema import HumanMessage, AIMessage, SystemMessage


## ---------- ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° ì²´í¬
db_path = './accomodation_data/'
db_initialized = os.path.exists(db_path)

if not db_initialized:
    # CSV íŒŒì¼ ì½ê¸°
    df = pd.read_csv('suksoDF.csv')

    # í…ìŠ¤íŠ¸ ë²¡í„°í™”: OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
    embeddings = OpenAIEmbeddings(
        model="text-embedding-ada-002"
    )

    db = Chroma(
        persist_directory=db_path,
        embedding_function=embeddings
    )

    # ë²¡í„°í™” ëœ ë°ì´í„°ë¥¼ DBì— ì €ì¥
    documents = [Document(page_content=f"ê´€ê´‘ì§€ëª…: {row['name']} ì£¼ì†Œ: {row['address']}, ì†Œê°œ: {row['overview']}, ìˆ™ì†Œì •ë³´: {row['generalInfo']}, ê°ì‹¤ì •ë³´: {row['roomInfo']}", metadata={"id": idx}) for idx, row in df.iterrows()]

    # ë¬¸ì„œ ë°°ì¹˜ë¥¼ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜
    def batch_documents(documents, batch_size):
        for i in range(0, len(documents), batch_size):
            yield documents[i:i + batch_size]

    # ë°°ì¹˜ í¬ê¸° ì„¤ì •
    batch_size = 100

    # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    for batch in batch_documents(documents, batch_size):
        try:
            db.add_documents(batch)
            db.persist()
            print(f"Batch of size {batch_size} added successfully.")
        except Exception as e:
            print(f"An error occurred: {e}")
else:
    # ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
    embeddings = OpenAIEmbeddings(
        model="text-embedding-ada-002"
    )

    db = Chroma(
        persist_directory=db_path,
        embedding_function=embeddings
    )

    # ë°ì´í„°ë² ì´ìŠ¤ê°€ ì˜¬ë°”ë¥´ê²Œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if db is None:
        raise ValueError("ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("ë°ì´í„°ë² ì´ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

llm = ChatOpenAI(
    model='gpt-4o-2024-08-06'
)

# 5. RetrievalQA ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm, 
    chain_type="map_reduce", 
    retriever=db.as_retriever()
)


## ---------- í”„ë¡¬í”„íŠ¸ ì„¤ê³„

def get_answer_from_db(user_query, chat_history):
    results = db.similarity_search(user_query, k=5)

    if not results:
        return "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    context = "\n".join([result.page_content for result in results])

    messages = chat_history.copy()
    messages.append(HumanMessage(content=f"ì‚¬ìš©ì ì§ˆë¬¸: {user_query}\n\nì°¸ê³ í•  ì •ë³´:\n{context}\n\nì´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.  {user_query}ì˜ ìš”êµ¬ì‚¬í•­ê³¼ {context}\n\nì˜ ì •ë³´ê°€ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ì œì™¸í•˜ê³  ë‹µë³€í•´ ì£¼ì„¸ìš”."))

    response = llm(messages)
    print(f"Response from ChatGPT: {response.content}")
    return response.content

# ---------- Streamlit êµ¬í˜„

st.set_page_config(page_title="ìƒë‹´ ì±—ë´‡", layout="wide", initial_sidebar_state="collapsed")
st.title("ğŸ¨ìˆ™ì†Œë´‡ğŸ¤–")

st.markdown(
    """
    <style>
    .chat-message {
        padding: 10px;
        border-radius: 10px;
        margin-bottom: 10px;
        display: inline-block;
    }
    .human-message {
        background-color: #F0F8FF; 
        text-align: left;
        color: black;
    }
    .ai-message {
        background-color: #E6E6FA; 
        text-align: left;
        color: black;
    }
    .stButton > button {
        background-color: yellow; 
        border: none;
        color: black;
        padding: 10px 20px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;
        margin: 4px 2px;
        cursor: pointer;
        border-radius: 4px;
    }
    .stButton > button:hover {
        background-color: #ffcc00; 
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ëŒ€í™” ê¸°ë¡ ì €ì¥
if 'messages' not in st.session_state:
    st.session_state['messages'] = [SystemMessage(content="ë„ˆëŠ” ì¹œì ˆí•˜ê³  ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ìˆ™ì†Œë¥¼ ì¶”ì²œí•´ì£¼ëŠ” ê°€ì´ë“œì•¼. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì„¸ì‹¬í•˜ê²Œ ë‹µë³€í•´ì¤˜.")]

with st.form("ì§ˆë¬¸í•˜ì„¸ìš”"):
    text = st.text_area("ì§ˆë¬¸ ì…ë ¥:", '')
    submitted = st.form_submit_button("ë³´ë‚´ê¸°")

if submitted and text:
    chat_history = st.session_state['messages']
    response = get_answer_from_db(text, chat_history)
    st.session_state['messages'].append(HumanMessage(content=text))
    st.session_state['messages'].append(AIMessage(content=response))

# ëŒ€í™” ê¸°ë¡ ì¶œë ¥
for message in st.session_state['messages']:
    if isinstance(message, HumanMessage):
        st.markdown(f"<div class='chat-message human-message'><strong>ğŸ‘©ğŸ»:</strong> {message.content}</div>", unsafe_allow_html=True)
    elif isinstance(message, AIMessage):
        st.markdown(f"<div class='chat-message ai-message'><strong>ğŸ¤–:</strong> {message.content}</div>", unsafe_allow_html=True)

if submitted and not text:
    st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
